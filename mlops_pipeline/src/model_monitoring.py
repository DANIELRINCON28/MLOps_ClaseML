"""
Sistema de Monitoreo y Detecci√≥n de Data Drift
Pipeline MLOps - Detecci√≥n de Fraude

Este m√≥dulo implementa un sistema completo de monitoreo que:
1. Carga datos hist√≥ricos (entrenamiento) y nuevos datos (producci√≥n)
2. Genera predicciones con el modelo entrenado
3. Calcula m√©tricas de data drift (KS, PSI, JS, Chi-cuadrado)
4. Detecta cambios en la distribuci√≥n de variables
5. Genera alertas autom√°ticas
6. Guarda resultados para visualizaci√≥n en Streamlit
"""

import pandas as pd
import numpy as np
import pickle
import json
from pathlib import Path
from datetime import datetime
from scipy import stats
from scipy.spatial.distance import jensenshannon
from scipy.stats import chi2_contingency
import warnings
warnings.filterwarnings('ignore')

# Obtener directorio ra√≠z del proyecto
PROJECT_ROOT = Path(__file__).resolve().parents[2]

class DataDriftMonitor:
    """
    Clase para monitorear data drift en datos de producci√≥n
    """
    
    def __init__(self, reference_data_path, model_path, preprocessor_path):
        """
        Inicializa el monitor de data drift
        
        Args:
            reference_data_path: Ruta a los datos de referencia (entrenamiento)
            model_path: Ruta al modelo entrenado
            preprocessor_path: Ruta al preprocesador
        """
        self.reference_data_path = Path(reference_data_path)
        self.model_path = Path(model_path)
        self.preprocessor_path = Path(preprocessor_path)
        
        # Umbrales para alertas
        self.thresholds = {
            'ks_stat': 0.1,      # Kolmogorov-Smirnov
            'psi': 0.2,          # Population Stability Index
            'js_divergence': 0.1, # Jensen-Shannon
            'chi2_pvalue': 0.05   # Chi-cuadrado
        }
        
        # Cargar datos de referencia y modelo
        self.load_reference_data()
        self.load_model()
        
    def load_reference_data(self):
        """Carga los datos de referencia (entrenamiento)"""
        print("üìÇ Cargando datos de referencia...")
        
        # Cargar datos procesados de entrenamiento usando rutas absolutas
        data_dir = PROJECT_ROOT / 'data' / 'processed'
        
        with open(data_dir / 'X_train.pkl', 'rb') as f:
            self.X_reference = pickle.load(f)
        
        with open(data_dir / 'y_train.pkl', 'rb') as f:
            self.y_reference = pickle.load(f)
        
        # Cargar datos originales para comparaci√≥n
        with open(data_dir / 'df_features_complete.pkl', 'rb') as f:
            self.df_reference = pickle.load(f)
        
        print(f"‚úÖ Datos de referencia cargados: {self.X_reference.shape}")
        
    def load_model(self):
        """Carga el modelo y preprocesador entrenados"""
        print("ü§ñ Cargando modelo entrenado...")
        
        with open(self.model_path, 'rb') as f:
            self.model = pickle.load(f)
        
        with open(self.preprocessor_path, 'rb') as f:
            self.preprocessor = pickle.load(f)
        
        print("‚úÖ Modelo y preprocesador cargados")
        
    def load_production_data(self, production_data_path):
        """
        Carga datos de producci√≥n para monitoreo
        
        Args:
            production_data_path: Ruta a los datos de producci√≥n
        """
        print(f"üìä Cargando datos de producci√≥n: {production_data_path}")
        
        # Cargar datos de producci√≥n (puede ser CSV o pickle)
        if str(production_data_path).endswith('.pkl'):
            with open(production_data_path, 'rb') as f:
                self.df_production = pickle.load(f)
        else:
            self.df_production = pd.read_csv(production_data_path)
        
        print(f"‚úÖ Datos de producci√≥n cargados: {self.df_production.shape}")
        
        return self.df_production
    
    def preprocess_production_data(self):
        """Preprocesa los datos de producci√≥n usando el preprocesador entrenado"""
        print("üîß Preprocesando datos de producci√≥n...")
        
        # Aplicar el mismo preprocesamiento que en entrenamiento
        # (Aqu√≠ asumimos que df_production tiene las mismas columnas que los datos de entrenamiento)
        
        # Separar features y target (si existe)
        if 'isFraud' in self.df_production.columns:
            X_prod = self.df_production.drop('isFraud', axis=1)
            y_prod = self.df_production['isFraud']
        else:
            X_prod = self.df_production.copy()
            y_prod = None
        
        # Aplicar preprocesador
        X_prod_processed = self.preprocessor.transform(X_prod)
        
        self.X_production = X_prod_processed
        self.y_production = y_prod
        
        print(f"‚úÖ Datos preprocesados: {self.X_production.shape}")
        
        return self.X_production, self.y_production
    
    def generate_predictions(self):
        """Genera predicciones para los datos de producci√≥n"""
        print("üéØ Generando predicciones...")
        
        # Predicciones binarias
        self.predictions = self.model.predict(self.X_production)
        
        # Probabilidades
        self.prediction_proba = self.model.predict_proba(self.X_production)[:, 1]
        
        print(f"‚úÖ Predicciones generadas: {len(self.predictions)}")
        print(f"   - Fraudes detectados: {self.predictions.sum()} ({self.predictions.sum()/len(self.predictions)*100:.2f}%)")
        
        return self.predictions, self.prediction_proba
    
    def calculate_ks_statistic(self, reference_col, production_col, col_name):
        """
        Calcula Kolmogorov-Smirnov test
        
        Args:
            reference_col: Columna de datos de referencia
            production_col: Columna de datos de producci√≥n
            col_name: Nombre de la columna
        
        Returns:
            dict con estad√≠sticos KS
        """
        # Remover NaN
        ref_clean = reference_col.dropna()
        prod_clean = production_col.dropna()
        
        # KS test
        ks_stat, p_value = stats.ks_2samp(ref_clean, prod_clean)
        
        # Determinar severidad
        if ks_stat < self.thresholds['ks_stat']:
            severity = 'low'
            status = '‚úÖ'
        elif ks_stat < self.thresholds['ks_stat'] * 2:
            severity = 'medium'
            status = '‚ö†Ô∏è'
        else:
            severity = 'high'
            status = 'üö®'
        
        return {
            'variable': col_name,
            'ks_statistic': ks_stat,
            'p_value': p_value,
            'severity': severity,
            'status': status,
            'drift_detected': ks_stat >= self.thresholds['ks_stat']
        }
    
    def calculate_psi(self, reference_col, production_col, col_name, bins=10):
        """
        Calcula Population Stability Index (PSI)
        
        PSI = sum((actual% - expected%) * ln(actual% / expected%))
        
        Interpretaci√≥n:
        - PSI < 0.1: Sin cambio significativo
        - 0.1 <= PSI < 0.2: Cambio moderado
        - PSI >= 0.2: Cambio significativo
        """
        # Remover NaN
        ref_clean = reference_col.dropna()
        prod_clean = production_col.dropna()
        
        # Crear bins basados en datos de referencia
        min_val = ref_clean.min()
        max_val = ref_clean.max()
        
        breakpoints = np.linspace(min_val, max_val, bins + 1)
        
        # Calcular distribuciones
        ref_counts, _ = np.histogram(ref_clean, bins=breakpoints)
        prod_counts, _ = np.histogram(prod_clean, bins=breakpoints)
        
        # Convertir a porcentajes
        ref_percents = ref_counts / len(ref_clean)
        prod_percents = prod_counts / len(prod_clean)
        
        # Evitar divisi√≥n por cero
        ref_percents = np.where(ref_percents == 0, 0.0001, ref_percents)
        prod_percents = np.where(prod_percents == 0, 0.0001, prod_percents)
        
        # Calcular PSI
        psi_values = (prod_percents - ref_percents) * np.log(prod_percents / ref_percents)
        psi = np.sum(psi_values)
        
        # Determinar severidad
        if psi < 0.1:
            severity = 'low'
            status = '‚úÖ'
        elif psi < 0.2:
            severity = 'medium'
            status = '‚ö†Ô∏è'
        else:
            severity = 'high'
            status = 'üö®'
        
        return {
            'variable': col_name,
            'psi': psi,
            'severity': severity,
            'status': status,
            'drift_detected': psi >= self.thresholds['psi']
        }
    
    def calculate_js_divergence(self, reference_col, production_col, col_name, bins=10):
        """
        Calcula Jensen-Shannon divergence
        
        JS = 0.5 * KL(P||M) + 0.5 * KL(Q||M)
        donde M = 0.5 * (P + Q)
        """
        # Remover NaN
        ref_clean = reference_col.dropna()
        prod_clean = production_col.dropna()
        
        # Crear bins
        min_val = min(ref_clean.min(), prod_clean.min())
        max_val = max(ref_clean.max(), prod_clean.max())
        
        breakpoints = np.linspace(min_val, max_val, bins + 1)
        
        # Calcular distribuciones
        ref_counts, _ = np.histogram(ref_clean, bins=breakpoints)
        prod_counts, _ = np.histogram(prod_clean, bins=breakpoints)
        
        # Normalizar
        ref_dist = ref_counts / ref_counts.sum()
        prod_dist = prod_counts / prod_counts.sum()
        
        # Evitar ceros
        ref_dist = np.where(ref_dist == 0, 1e-10, ref_dist)
        prod_dist = np.where(prod_dist == 0, 1e-10, prod_dist)
        
        # Calcular JS divergence
        js_div = jensenshannon(ref_dist, prod_dist)
        
        # Determinar severidad
        if js_div < self.thresholds['js_divergence']:
            severity = 'low'
            status = '‚úÖ'
        elif js_div < self.thresholds['js_divergence'] * 2:
            severity = 'medium'
            status = '‚ö†Ô∏è'
        else:
            severity = 'high'
            status = 'üö®'
        
        return {
            'variable': col_name,
            'js_divergence': js_div,
            'severity': severity,
            'status': status,
            'drift_detected': js_div >= self.thresholds['js_divergence']
        }
    
    def calculate_chi2_test(self, reference_col, production_col, col_name):
        """
        Calcula Chi-cuadrado test para variables categ√≥ricas
        """
        # Obtener categor√≠as √∫nicas
        all_categories = set(reference_col.unique()) | set(production_col.unique())
        
        # Contar frecuencias
        ref_counts = reference_col.value_counts().reindex(all_categories, fill_value=0)
        prod_counts = production_col.value_counts().reindex(all_categories, fill_value=0)
        
        # Crear tabla de contingencia
        contingency_table = np.array([ref_counts, prod_counts])
        
        # Chi-cuadrado test
        chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)
        
        # Determinar severidad
        if p_value >= self.thresholds['chi2_pvalue']:
            severity = 'low'
            status = '‚úÖ'
        elif p_value >= self.thresholds['chi2_pvalue'] / 2:
            severity = 'medium'
            status = '‚ö†Ô∏è'
        else:
            severity = 'high'
            status = 'üö®'
        
        return {
            'variable': col_name,
            'chi2_statistic': chi2_stat,
            'p_value': p_value,
            'severity': severity,
            'status': status,
            'drift_detected': p_value < self.thresholds['chi2_pvalue']
        }
    
    def detect_drift(self, sample_size=None):
        """
        Detecta data drift comparando datos de referencia vs producci√≥n
        
        Args:
            sample_size: Tama√±o de muestra para an√°lisis (None = todos los datos)
        """
        print("\n" + "="*80)
        print("üîç INICIANDO DETECCI√ìN DE DATA DRIFT")
        print("="*80)
        
        drift_results = []
        
        # Muestreo si se especifica
        if sample_size:
            df_ref_sample = self.df_reference.sample(min(sample_size, len(self.df_reference)), random_state=42)
            df_prod_sample = self.df_production.sample(min(sample_size, len(self.df_production)), random_state=42)
        else:
            df_ref_sample = self.df_reference
            df_prod_sample = self.df_production
        
        # Seleccionar columnas num√©ricas para an√°lisis
        numeric_columns = df_ref_sample.select_dtypes(include=[np.number]).columns
        numeric_columns = [col for col in numeric_columns if col in df_prod_sample.columns and col != 'isFraud']
        
        print(f"\nüìä Analizando {len(numeric_columns)} variables num√©ricas...")
        
        for col in numeric_columns:
            if col in df_prod_sample.columns:
                print(f"\n   Analizando: {col}")
                
                ref_col = df_ref_sample[col]
                prod_col = df_prod_sample[col]
                
                # Calcular m√©tricas de drift
                ks_result = self.calculate_ks_statistic(ref_col, prod_col, col)
                psi_result = self.calculate_psi(ref_col, prod_col, col)
                js_result = self.calculate_js_divergence(ref_col, prod_col, col)
                
                # Combinar resultados
                drift_info = {
                    'variable': col,
                    'tipo': 'num√©rica',
                    'ks_statistic': ks_result['ks_statistic'],
                    'ks_p_value': ks_result['p_value'],
                    'psi': psi_result['psi'],
                    'js_divergence': js_result['js_divergence'],
                    'drift_detected': (ks_result['drift_detected'] or 
                                     psi_result['drift_detected'] or 
                                     js_result['drift_detected']),
                    'severity': max([ks_result['severity'], psi_result['severity'], js_result['severity']],
                                  key=lambda x: {'low': 0, 'medium': 1, 'high': 2}[x]),
                    'ref_mean': float(ref_col.mean()),
                    'prod_mean': float(prod_col.mean()),
                    'ref_std': float(ref_col.std()),
                    'prod_std': float(prod_col.std()),
                    'mean_change_%': float((prod_col.mean() - ref_col.mean()) / ref_col.mean() * 100) if ref_col.mean() != 0 else 0
                }
                
                drift_results.append(drift_info)
                
                # Mostrar estado
                status = 'üö®' if drift_info['severity'] == 'high' else '‚ö†Ô∏è' if drift_info['severity'] == 'medium' else '‚úÖ'
                print(f"      {status} KS={ks_result['ks_statistic']:.4f}, PSI={psi_result['psi']:.4f}, JS={js_result['js_divergence']:.4f}")
        
        # Analizar variables categ√≥ricas
        categorical_columns = df_ref_sample.select_dtypes(include=['object', 'category']).columns
        categorical_columns = [col for col in categorical_columns if col in df_prod_sample.columns]
        
        if len(categorical_columns) > 0:
            print(f"\nüìä Analizando {len(categorical_columns)} variables categ√≥ricas...")
            
            for col in categorical_columns:
                print(f"\n   Analizando: {col}")
                
                ref_col = df_ref_sample[col]
                prod_col = df_prod_sample[col]
                
                chi2_result = self.calculate_chi2_test(ref_col, prod_col, col)
                
                drift_info = {
                    'variable': col,
                    'tipo': 'categ√≥rica',
                    'chi2_statistic': chi2_result['chi2_statistic'],
                    'chi2_p_value': chi2_result['p_value'],
                    'drift_detected': chi2_result['drift_detected'],
                    'severity': chi2_result['severity'],
                    'ref_categories': len(ref_col.unique()),
                    'prod_categories': len(prod_col.unique())
                }
                
                drift_results.append(drift_info)
                
                status = 'üö®' if drift_info['severity'] == 'high' else '‚ö†Ô∏è' if drift_info['severity'] == 'medium' else '‚úÖ'
                print(f"      {status} Chi2={chi2_result['chi2_statistic']:.4f}, p-value={chi2_result['p_value']:.4f}")
        
        self.drift_results = pd.DataFrame(drift_results)
        
        return self.drift_results
    
    def generate_alerts(self):
        """Genera alertas basadas en los resultados de drift"""
        print("\n" + "="*80)
        print("üö® GENERACI√ìN DE ALERTAS")
        print("="*80)
        
        alerts = []
        
        # Alertas por severidad alta
        high_severity = self.drift_results[self.drift_results['severity'] == 'high']
        
        if len(high_severity) > 0:
            alert = {
                'timestamp': datetime.now().isoformat(),
                'level': 'CR√çTICO',
                'message': f'üö® ALERTA CR√çTICA: {len(high_severity)} variables con drift severo detectado',
                'variables': high_severity['variable'].tolist(),
                'recommendation': 'ACCI√ìN INMEDIATA REQUERIDA: Considerar reentrenamiento del modelo',
                'details': high_severity.to_dict('records')
            }
            alerts.append(alert)
            print(f"\nüö® CR√çTICO: {len(high_severity)} variables con drift severo")
            print(f"   Variables: {', '.join(high_severity['variable'].tolist())}")
        
        # Alertas por severidad media
        medium_severity = self.drift_results[self.drift_results['severity'] == 'medium']
        
        if len(medium_severity) > 0:
            alert = {
                'timestamp': datetime.now().isoformat(),
                'level': 'ADVERTENCIA',
                'message': f'‚ö†Ô∏è ADVERTENCIA: {len(medium_severity)} variables con drift moderado',
                'variables': medium_severity['variable'].tolist(),
                'recommendation': 'Monitorear de cerca estas variables en los pr√≥ximos per√≠odos',
                'details': medium_severity.to_dict('records')
            }
            alerts.append(alert)
            print(f"\n‚ö†Ô∏è ADVERTENCIA: {len(medium_severity)} variables con drift moderado")
            print(f"   Variables: {', '.join(medium_severity['variable'].tolist())}")
        
        # Resumen general
        drift_detected = self.drift_results[self.drift_results['drift_detected'] == True]
        
        summary_alert = {
            'timestamp': datetime.now().isoformat(),
            'level': 'INFO',
            'message': f'üìä Resumen: {len(drift_detected)}/{len(self.drift_results)} variables con drift detectado',
            'total_variables': len(self.drift_results),
            'drift_detected': len(drift_detected),
            'high_severity': len(high_severity),
            'medium_severity': len(medium_severity),
            'recommendation': 'Revisar dashboard de monitoreo para m√°s detalles'
        }
        alerts.append(summary_alert)
        
        print(f"\nüìä RESUMEN GENERAL:")
        print(f"   Total variables analizadas: {len(self.drift_results)}")
        print(f"   Variables con drift: {len(drift_detected)}")
        print(f"   Severidad alta: {len(high_severity)}")
        print(f"   Severidad media: {len(medium_severity)}")
        
        self.alerts = alerts
        
        return alerts
    
    def save_results(self, output_dir='../../outputs/monitoring'):
        """Guarda los resultados del monitoreo"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        print(f"\nüíæ Guardando resultados en {output_path}...")
        
        # Guardar resultados de drift
        drift_file = output_path / f'drift_results_{timestamp}.csv'
        self.drift_results.to_csv(drift_file, index=False)
        print(f"   ‚úÖ Drift results: {drift_file}")
        
        # Guardar alertas
        alerts_file = output_path / f'alerts_{timestamp}.json'
        with open(alerts_file, 'w', encoding='utf-8') as f:
            json.dump(self.alerts, f, indent=2, ensure_ascii=False)
        print(f"   ‚úÖ Alerts: {alerts_file}")
        
        # Guardar predicciones
        predictions_df = pd.DataFrame({
            'prediction': self.predictions,
            'prediction_proba': self.prediction_proba
        })
        
        # Agregar datos originales si est√°n disponibles
        if hasattr(self, 'df_production'):
            predictions_df = pd.concat([self.df_production.reset_index(drop=True), predictions_df], axis=1)
        
        predictions_file = output_path / f'predictions_{timestamp}.csv'
        predictions_df.to_csv(predictions_file, index=False)
        print(f"   ‚úÖ Predictions: {predictions_file}")
        
        # Guardar resumen para Streamlit
        summary = {
            'timestamp': timestamp,
            'total_variables': len(self.drift_results),
            'drift_detected': int(self.drift_results['drift_detected'].sum()),
            'high_severity': int((self.drift_results['severity'] == 'high').sum()),
            'medium_severity': int((self.drift_results['severity'] == 'medium').sum()),
            'low_severity': int((self.drift_results['severity'] == 'low').sum()),
            'predictions': {
                'total': len(self.predictions),
                'fraud_detected': int(self.predictions.sum()),
                'fraud_rate': float(self.predictions.sum() / len(self.predictions) * 100)
            }
        }
        
        summary_file = output_path / 'latest_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2)
        print(f"   ‚úÖ Summary: {summary_file}")
        
        print("\n‚úÖ Todos los resultados guardados exitosamente")
        
        return {
            'drift_results': drift_file,
            'alerts': alerts_file,
            'predictions': predictions_file,
            'summary': summary_file
        }


def main():
    """Funci√≥n principal para ejecutar el monitoreo"""
    print("\n" + "="*80)
    print("üîç SISTEMA DE MONITOREO Y DETECCI√ìN DE DATA DRIFT")
    print("Pipeline MLOps - Detecci√≥n de Fraude")
    print("="*80 + "\n")
    
    # Rutas a los archivos
    model_path = '../../models/xgboost_model.pkl'
    preprocessor_path = '../../data/processed/preprocessor.pkl'
    reference_data_path = '../../data/processed/df_features_complete.pkl'
    
    # Crear monitor
    monitor = DataDriftMonitor(
        reference_data_path=reference_data_path,
        model_path=model_path,
        preprocessor_path=preprocessor_path
    )
    
    # Cargar datos de producci√≥n (simulaci√≥n usando datos de test)
    # En producci√≥n real, esto vendr√≠a de una base de datos o API
    print("\nüìä Simulando datos de producci√≥n...")
    print("   (En producci√≥n, estos datos vendr√≠an de la base de datos en tiempo real)")
    
    # Usar los datos de test como "producci√≥n"
    production_data = monitor.load_production_data('../../data/processed/df_features_complete.pkl')
    
    # Tomar una muestra para simular datos nuevos
    production_sample = production_data.sample(frac=0.3, random_state=42)
    
    # Simular algunos cambios en los datos (para demostrar drift)
    print("\n‚ö° Simulando cambios en la distribuci√≥n de datos...")
    production_sample_modified = production_sample.copy()
    
    # Modificar algunas columnas para inducir drift
    if 'amount' in production_sample_modified.columns:
        production_sample_modified['amount'] = production_sample_modified['amount'] * 1.2  # 20% m√°s
    
    if 'oldbalanceOrg' in production_sample_modified.columns:
        production_sample_modified['oldbalanceOrg'] = production_sample_modified['oldbalanceOrg'] * 0.8  # 20% menos
    
    # Guardar temporalmente
    temp_prod_path = '../../data/processed/temp_production_data.csv'
    production_sample_modified.to_csv(temp_prod_path, index=False)
    
    # Cargar datos de producci√≥n modificados
    monitor.df_production = production_sample_modified
    
    # Preprocesar datos de producci√≥n
    monitor.preprocess_production_data()
    
    # Generar predicciones
    monitor.generate_predictions()
    
    # Detectar drift (muestreo de 5000 registros para an√°lisis m√°s r√°pido)
    drift_results = monitor.detect_drift(sample_size=5000)
    
    # Generar alertas
    alerts = monitor.generate_alerts()
    
    # Guardar resultados
    saved_files = monitor.save_results()
    
    print("\n" + "="*80)
    print("‚úÖ MONITOREO COMPLETADO EXITOSAMENTE")
    print("="*80)
    print("\nüìä Los resultados est√°n disponibles para visualizaci√≥n en Streamlit")
    print("   Ejecuta: streamlit run app_monitoring.py")
    print("\n")


if __name__ == "__main__":
    main()

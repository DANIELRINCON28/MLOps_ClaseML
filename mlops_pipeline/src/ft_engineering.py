"""
Feature Engineering Pipeline para DetecciÃ³n de Fraude
======================================================

Este script implementa el pipeline completo de ingenierÃ­a de caracterÃ­sticas
siguiendo las mejores prÃ¡cticas de MLOps y sklearn.

OBJETIVO:
---------
Transformar el dataset original en un conjunto de features Ã³ptimo para la 
detecciÃ³n de fraude, aplicando tÃ©cnicas de:
- CreaciÃ³n de features derivadas
- Escalado y normalizaciÃ³n
- CodificaciÃ³n de variables categÃ³ricas
- ImputaciÃ³n de valores faltantes

FLUJO DE TRANSFORMACIÃ“N:
------------------------
1. CARGA DE DATOS
   â””â”€> Lectura del dataset original desde pickle/CSV

2. CREACIÃ“N DE FEATURES DERIVADAS
   â”œâ”€> Features de Balance (diferencias, errores, ratios)
   â”œâ”€> Features Binarios (tipo de entidad, balances en cero)
   â”œâ”€> Features Temporales (hora, dÃ­a, fin de semana, noche)
   â”œâ”€> Features de Tipo (transacciones propensas a fraude)
   â””â”€> Features de Magnitud (categorizaciÃ³n de montos)

3. PREPARACIÃ“N PARA MODELADO
   â”œâ”€> SeparaciÃ³n de features (X) y target (y)
   â”œâ”€> DivisiÃ³n en conjuntos de entrenamiento y prueba
   â””â”€> EstratificaciÃ³n para mantener distribuciÃ³n de clases

4. CONSTRUCCIÃ“N DE PIPELINES
   â”œâ”€> Pipeline NumÃ©rico:
   â”‚   â”œâ”€> SimpleImputer (strategy='median')
   â”‚   â””â”€> RobustScaler (robusto a outliers)
   â”œâ”€> Pipeline CategÃ³rico:
   â”‚   â”œâ”€> SimpleImputer (strategy='most_frequent')
   â”‚   â””â”€> OneHotEncoder (drop='first', handle_unknown='ignore')
   â””â”€> ColumnTransformer (combina ambos pipelines)

5. AJUSTE Y TRANSFORMACIÃ“N
   â”œâ”€> Fit en datos de entrenamiento
   â”œâ”€> Transform en datos de entrenamiento y prueba
   â””â”€> GeneraciÃ³n de DataFrames con nombres de features

6. GUARDADO DE ARTEFACTOS
   â”œâ”€> Datasets procesados (X_train, X_test, y_train, y_test)
   â”œâ”€> Preprocesador ajustado (para uso en producciÃ³n)
   â”œâ”€> Dataset completo con features
   â””â”€> Metadatos del proceso

DECISIONES DE DISEÃ‘O:
---------------------
âœ“ RobustScaler vs StandardScaler: MÃ¡s robusto ante outliers
âœ“ OneHotEncoder con drop='first': Evita multicolinealidad
âœ“ ImputaciÃ³n con mediana: Robusto ante outliers
âœ“ EstratificaciÃ³n: Mantiene proporciÃ³n de fraudes en train/test
âœ“ Pipeline Pattern: Facilita reproducibilidad y deployment

OUTPUTS GENERADOS:
------------------
- data/processed/X_train.pkl: Features de entrenamiento
- data/processed/X_test.pkl: Features de prueba
- data/processed/y_train.pkl: Target de entrenamiento
- data/processed/y_test.pkl: Target de prueba
- data/processed/preprocessor.pkl: Pipeline completo ajustado
- data/processed/df_features_complete.pkl: Dataset con todas las features
- data/processed/feature_engineering_metadata.pkl: Metadatos

Autores: MLOps Team
Fecha: Noviembre 2025
VersiÃ³n: 2.0
"""

import pandas as pd
import numpy as np
import pickle
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import warnings

warnings.filterwarnings('ignore')


class FraudFeatureEngineering:
    """
    Clase para manejar toda la ingenierÃ­a de caracterÃ­sticas 
    del proyecto de detecciÃ³n de fraude.
    """
    
    def __init__(self, data_path='../../data/processed/df_original.pkl'):
        """
        Inicializa el ingeniero de caracterÃ­sticas.
        
        Parameters:
        -----------
        data_path : str
            Ruta al archivo de datos
        """
        self.data_path = data_path
        self.df = None
        self.df_features = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.preprocessor = None
        
        print("=" * 80)
        print("FRAUD DETECTION - FEATURE ENGINEERING PIPELINE")
        print("=" * 80)
    
    
    def load_data(self):
        """Carga los datos desde el archivo pickle."""
        print("\nğŸ”„ Cargando datos...")
        
        try:
            self.df = pd.read_pickle(self.data_path)
            print(f"âœ… Datos cargados: {self.df.shape[0]:,} filas x {self.df.shape[1]} columnas")
        except:
            # Intentar desde CSV si pickle no existe
            csv_path = '../../Base_datos.csv'
            self.df = pd.read_csv(csv_path)
            print(f"âœ… Datos cargados desde CSV: {self.df.shape[0]:,} filas x {self.df.shape[1]} columnas")
        
        return self.df
    
    
    def create_features(self):
        """
        Crea nuevas caracterÃ­sticas derivadas basadas en el anÃ¡lisis exploratorio.
        
        DECISIONES DE INGENIERÃA:
        -------------------------
        Todas las features fueron diseÃ±adas basÃ¡ndose en:
        1. AnÃ¡lisis exploratorio de datos (EDA)
        2. Conocimiento del dominio de fraude financiero
        3. CorrelaciÃ³n con la variable objetivo
        
        CATEGORÃAS DE FEATURES:
        -----------------------
        1. BALANCE: Detectar inconsistencias en transacciones
        2. BINARIOS: Identificar patrones de entidades y balances
        3. RATIOS: Proporciones relativas de montos
        4. TEMPORALES: Patrones de tiempo sospechosos
        5. TIPO: Transacciones propensas a fraude
        6. MAGNITUD: CategorizaciÃ³n de montos
        
        Returns:
        --------
        df_features : DataFrame
            Dataset con features originales y derivadas
        """
        print("\nğŸ”§ Creando nuevas caracterÃ­sticas...")
        
        self.df_features = self.df.copy()
        
        # ========================================================================
        # 1. FEATURES DE DIFERENCIA DE BALANCES
        # ========================================================================
        # DECISIÃ“N: Detectar inconsistencias matemÃ¡ticas que pueden indicar fraude
        # Las transacciones legÃ­timas deberÃ­an tener balance_diff â‰ˆ amount
        print("  ğŸ“Š Creando features de balance...")
        
        self.df_features['balance_diff_orig'] = (
            self.df_features['oldbalanceOrg'] - self.df_features['newbalanceOrig']
        )
        
        self.df_features['balance_diff_dest'] = (
            self.df_features['newbalanceDest'] - self.df_features['oldbalanceDest']
        )
        
        # Error = diferencia absoluta entre el cambio de balance y el monto
        # Valores altos sugieren transacciones sospechosas
        self.df_features['error_balance_orig'] = np.abs(
            self.df_features['balance_diff_orig'] - self.df_features['amount']
        )
        
        self.df_features['error_balance_dest'] = np.abs(
            self.df_features['balance_diff_dest'] - self.df_features['amount']
        )
        
        self.df_features['error_balance_total'] = (
            self.df_features['error_balance_orig'] + self.df_features['error_balance_dest']
        )
        
        
        # ========================================================================
        # 2. FEATURES BINARIOS
        # ========================================================================
        # DECISIÃ“N: Identificar tipos de entidades y comportamientos sospechosos
        # Los merchants (M) tienen patrones diferentes a clientes (C)
        print("  ğŸ“Š Creando features binarios...")
        
        self.df_features['orig_is_merchant'] = (
            self.df_features['nameOrig'].str[0] == 'M'
        ).astype(int)
        
        self.df_features['dest_is_merchant'] = (
            self.df_features['nameDest'].str[0] == 'M'
        ).astype(int)
        
        # Balances en cero pueden indicar vaciado de cuenta (sospechoso)
        self.df_features['orig_balance_zero_after'] = (
            self.df_features['newbalanceOrig'] == 0
        ).astype(int)
        
        self.df_features['dest_balance_zero_after'] = (
            self.df_features['newbalanceDest'] == 0
        ).astype(int)
        
        self.df_features['orig_balance_zero_before'] = (
            self.df_features['oldbalanceOrg'] == 0
        ).astype(int)
        
        self.df_features['dest_balance_zero_before'] = (
            self.df_features['oldbalanceDest'] == 0
        ).astype(int)
        
        
        # ========================================================================
        # 3. FEATURES DE RATIOS
        # ========================================================================
        # DECISIÃ“N: Transacciones grandes relativas al balance son sospechosas
        # Ratios > 1 indican que se transfiere mÃ¡s del balance disponible
        print("  ğŸ“Š Creando features de ratios...")
        
        # +1 en denominador para evitar divisiÃ³n por cero
        self.df_features['amount_to_oldbalance_orig_ratio'] = (
            self.df_features['amount'] / (self.df_features['oldbalanceOrg'] + 1)
        )
        
        self.df_features['amount_to_oldbalance_dest_ratio'] = (
            self.df_features['amount'] / (self.df_features['oldbalanceDest'] + 1)
        )
        
        self.df_features['balance_ratio_orig'] = (
            self.df_features['newbalanceOrig'] / (self.df_features['oldbalanceOrg'] + 1)
        )
        
        self.df_features['balance_ratio_dest'] = (
            self.df_features['newbalanceDest'] / (self.df_features['oldbalanceDest'] + 1)
        )
        
        
        # ========================================================================
        # 4. FEATURES TEMPORALES
        # ========================================================================
        # DECISIÃ“N: Fraudes pueden ocurrir en horarios especÃ­ficos
        # Transacciones nocturnas o de fin de semana pueden ser mÃ¡s riesgosas
        print("  ğŸ“Š Creando features temporales...")
        
        self.df_features['hour_of_day'] = self.df_features['step'] % 24
        self.df_features['day_of_month'] = (self.df_features['step'] // 24) + 1
        
        # Fin de semana: dÃ­as 6 y 7 de cada semana
        self.df_features['is_weekend'] = (
            ((self.df_features['step'] // 24) % 7) >= 5
        ).astype(int)
        
        # Horario nocturno: 22:00 - 06:00
        self.df_features['is_night'] = (
            (self.df_features['hour_of_day'] >= 22) | 
            (self.df_features['hour_of_day'] <= 6)
        ).astype(int)
        
        
        # ========================================================================
        # 5. FEATURES DE TIPO DE TRANSACCIÃ“N
        # ========================================================================
        # DECISIÃ“N: SegÃºn EDA, fraudes SOLO ocurren en TRANSFER y CASH_OUT
        # Este es un indicador muy fuerte
        print("  ğŸ“Š Creando features de tipo de transacciÃ³n...")
        
        fraud_types = ['TRANSFER', 'CASH_OUT']
        self.df_features['is_fraud_type'] = (
            self.df_features['type'].isin(fraud_types)
        ).astype(int)
        
        
        # ========================================================================
        # 6. FEATURES DE MAGNITUD
        # ========================================================================
        # DECISIÃ“N: Transacciones muy grandes (>200k) son seÃ±aladas por el sistema
        # La categorizaciÃ³n facilita el anÃ¡lisis no lineal
        print("  ğŸ“Š Creando features de magnitud...")
        
        # Flag del sistema para transacciones > 200,000
        self.df_features['is_large_transaction'] = (
            self.df_features['amount'] > 200000
        ).astype(int)
        
        # CategorizaciÃ³n de montos (facilita anÃ¡lisis no lineal)
        self.df_features['amount_category'] = pd.cut(
            self.df_features['amount'],
            bins=[0, 1000, 10000, 100000, float('inf')],
            labels=['small', 'medium', 'large', 'very_large']
        )
        
        
        new_features_count = len(self.df_features.columns) - len(self.df.columns)
        print(f"âœ… {new_features_count} nuevas features creadas")
        print(f"ğŸ“Š Total de columnas: {len(self.df_features.columns)}")
        
        # Resumen de features creadas
        print("\nğŸ“‹ RESUMEN DE FEATURES CREADAS:")
        print("  â€¢ Balance Features: 5")
        print("  â€¢ Binary Features: 6")
        print("  â€¢ Ratio Features: 4")
        print("  â€¢ Temporal Features: 4")
        print("  â€¢ Type Features: 1")
        print("  â€¢ Magnitude Features: 2")
        print(f"  TOTAL: {new_features_count} features")
        
        return self.df_features
    
    
    def prepare_for_modeling(self, test_size=0.2, random_state=42):
        """
        Prepara los datos para el modelado.
        """
        print("\nğŸ“¦ Preparando datos para modelado...")
        
        columns_to_drop = [
            'nameOrig',
            'nameDest',
            'isFraud',
            'isFlaggedFraud'
        ]
        
        X = self.df_features.drop(columns=columns_to_drop)
        y = self.df_features['isFraud']
        
        print(f"  ğŸ“Š Features (X): {X.shape}")
        print(f"  ğŸ¯ Target (y): {y.shape}")
        print(f"  ğŸ“Š DistribuciÃ³n de clases:")
        print(f"     - No Fraude: {(y==0).sum():,} ({(y==0).sum()/len(y)*100:.2f}%)")
        print(f"     - Fraude: {(y==1).sum():,} ({(y==1).sum()/len(y)*100:.2f}%)")
        
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, 
            test_size=test_size, 
            random_state=random_state,
            stratify=y
        )
        
        print(f"\nâœ… DivisiÃ³n completada:")
        print(f"  ğŸ“Š Train: {self.X_train.shape[0]:,} muestras")
        print(f"  ğŸ“Š Test: {self.X_test.shape[0]:,} muestras")
        
        return self.X_train, self.X_test, self.y_train, self.y_test
    
    
    def build_preprocessor(self):
        """
        Construye el pipeline de preprocesamiento usando ColumnTransformer.
        
        ARQUITECTURA DEL PIPELINE:
        --------------------------
        
        ColumnTransformer
        â”œâ”€> numeric_transformer (Pipeline)
        â”‚   â”œâ”€> SimpleImputer(strategy='median')
        â”‚   â”‚   â””â”€ Imputa valores faltantes con la mediana
        â”‚   â”‚      DECISIÃ“N: Mediana es robusta ante outliers
        â”‚   â””â”€> RobustScaler()
        â”‚       â””â”€ Escala usando IQR, robusto ante outliers
        â”‚          DECISIÃ“N: Preferido sobre StandardScaler por alta
        â”‚                    presencia de outliers en datos financieros
        â”‚
        â””â”€> categorical_transformer (Pipeline)
            â”œâ”€> SimpleImputer(strategy='most_frequent')
            â”‚   â””â”€ Imputa con el valor mÃ¡s frecuente
            â”‚      DECISIÃ“N: Apropiado para categÃ³ricas
            â””â”€> OneHotEncoder(drop='first', handle_unknown='ignore')
                â””â”€ Codifica categÃ³ricas en binarias
                   DECISIÃ“N: drop='first' evita multicolinealidad
                            handle_unknown='ignore' maneja categorÃ­as nuevas
        
        TRANSFORMACIONES APLICADAS:
        ---------------------------
        1. IMPUTACIÃ“N:
           - NumÃ©ricas: mediana (robusto a outliers)
           - CategÃ³ricas: moda (mÃ¡s frecuente)
        
        2. ESCALADO:
           - RobustScaler: Usa IQR en lugar de desviaciÃ³n estÃ¡ndar
           - Rango: [Q1 - 1.5*IQR, Q3 + 1.5*IQR]
           - Ventaja: No afectado por outliers extremos
        
        3. CODIFICACIÃ“N:
           - OneHotEncoder: Variables categÃ³ricas â†’ binarias
           - drop='first': Evita dummy variable trap
           - handle_unknown='ignore': ProducciÃ³n-ready
        
        Returns:
        --------
        preprocessor : ColumnTransformer
            Pipeline completo de preprocesamiento
        """
        print("\nğŸ—ï¸ Construyendo pipeline de preprocesamiento...")
        
        # Identificar variables numÃ©ricas y categÃ³ricas
        numeric_features = self.X_train.select_dtypes(
            include=['int8', 'int16', 'int32', 'int64', 'float32', 'float64']
        ).columns.tolist()
        
        categorical_features = self.X_train.select_dtypes(
            include=['object', 'category']
        ).columns.tolist()
        
        print(f"  ğŸ“Š Variables numÃ©ricas: {len(numeric_features)}")
        print(f"  ğŸ“Š Variables categÃ³ricas: {len(categorical_features)}")
        
        # ========================================================================
        # PIPELINE NUMÃ‰RICO
        # ========================================================================
        # Step 1: ImputaciÃ³n con mediana (robusto a outliers)
        # Step 2: Escalado robusto (usa IQR, no afectado por outliers)
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', RobustScaler())
        ])
        
        print("  âœ… Pipeline numÃ©rico configurado:")
        print("     - ImputaciÃ³n: mediana")
        print("     - Escalado: RobustScaler (IQR-based)")
        
        # ========================================================================
        # PIPELINE CATEGÃ“RICO
        # ========================================================================
        # Step 1: ImputaciÃ³n con moda
        # Step 2: One-Hot Encoding (drop='first' para evitar multicolinealidad)
        categoric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))
        ])
        
        print("  âœ… Pipeline categÃ³rico configurado:")
        print("     - ImputaciÃ³n: moda (most_frequent)")
        print("     - CodificaciÃ³n: OneHotEncoder (drop='first')")
        
        # ========================================================================
        # COLUMN TRANSFORMER
        # ========================================================================
        # Combina ambos pipelines aplicÃ¡ndolos a las columnas correspondientes
        self.preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features),
                ('cat', categoric_transformer, categorical_features)
            ],
            remainder='passthrough'  # Mantiene columnas no especificadas
        )
        
        print("âœ… ColumnTransformer construido")
        print("   Combina pipelines numÃ©rico y categÃ³rico")
        
        return self.preprocessor
    
    
    def fit_transform_data(self):
        """
        Ajusta el preprocesador y transforma los datos.
        """
        print("\nğŸ”„ Ajustando y transformando datos...")
        
        X_train_processed = self.preprocessor.fit_transform(self.X_train)
        X_test_processed = self.preprocessor.transform(self.X_test)
        
        print(f"âœ… Datos transformados")
        print(f"  ğŸ“Š X_train procesado: {X_train_processed.shape}")
        print(f"  ğŸ“Š X_test procesado: {X_test_processed.shape}")
        
        feature_names = self._get_feature_names()
        
        X_train_processed_df = pd.DataFrame(
            X_train_processed, 
            columns=feature_names,
            index=self.X_train.index
        )
        
        X_test_processed_df = pd.DataFrame(
            X_test_processed, 
            columns=feature_names,
            index=self.X_test.index
        )
        
        return X_train_processed_df, X_test_processed_df
    
    
    def _get_feature_names(self):
        """Obtiene los nombres de las features despuÃ©s de la transformaciÃ³n."""
        feature_names = []
        
        num_features = self.preprocessor.transformers_[0][2]
        feature_names.extend(num_features)
        
        if len(self.preprocessor.transformers_[1][2]) > 0:
            cat_encoder = self.preprocessor.transformers_[1][1].named_steps['encoder']
            cat_features = cat_encoder.get_feature_names_out(
                self.preprocessor.transformers_[1][2]
            )
            feature_names.extend(cat_features)
        
        return feature_names
    
    
    def save_artifacts(self, output_dir='../../data/processed'):
        """
        Guarda todos los artefactos.
        """
        print("\nğŸ’¾ Guardando artefactos...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        self.X_train.to_pickle(f'{output_dir}/X_train.pkl')
        self.X_test.to_pickle(f'{output_dir}/X_test.pkl')
        self.y_train.to_pickle(f'{output_dir}/y_train.pkl')
        self.y_test.to_pickle(f'{output_dir}/y_test.pkl')
        
        print(f"  âœ… Datasets guardados")
        
        with open(f'{output_dir}/preprocessor.pkl', 'wb') as f:
            pickle.dump(self.preprocessor, f)
        print(f"  âœ… Preprocesador guardado")
        
        self.df_features.to_pickle(f'{output_dir}/df_features_complete.pkl')
        print(f"  âœ… Dataset completo guardado")
        
        metadata = {
            'n_features': self.X_train.shape[1],
            'n_samples_train': self.X_train.shape[0],
            'n_samples_test': self.X_test.shape[0],
            'feature_names': list(self.X_train.columns),
            'class_distribution_train': {
                'no_fraud': int((self.y_train == 0).sum()),
                'fraud': int((self.y_train == 1).sum())
            },
            'class_distribution_test': {
                'no_fraud': int((self.y_test == 0).sum()),
                'fraud': int((self.y_test == 1).sum())
            }
        }
        
        with open(f'{output_dir}/feature_engineering_metadata.pkl', 'wb') as f:
            pickle.dump(metadata, f)
        print(f"  âœ… Metadatos guardados")
        
        return metadata


def main():
    """FunciÃ³n principal."""
    
    fe = FraudFeatureEngineering()
    fe.load_data()
    fe.create_features()
    fe.prepare_for_modeling(test_size=0.2, random_state=42)
    fe.build_preprocessor()
    X_train_processed, X_test_processed = fe.fit_transform_data()
    metadata = fe.save_artifacts()
    
    print("\n" + "=" * 80)
    print("FEATURE ENGINEERING COMPLETADO âœ…")
    print("=" * 80)
    print(f"\nğŸ“Š Total de features: {metadata['n_features']}")
    print(f"ğŸ“Š Muestras de entrenamiento: {metadata['n_samples_train']:,}")
    print(f"ğŸ“Š Muestras de prueba: {metadata['n_samples_test']:,}")
    print("\nâ¡ï¸ Siguiente paso: Model Training")
    print("=" * 80)
    
    return fe, X_train_processed, X_test_processed


if __name__ == "__main__":
    fe, X_train_processed, X_test_processed = main()